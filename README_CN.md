# ICMA: å¤§è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡åˆ†å­å­¦ä¹ æ¡†æ¶

ICMA (In-Context Molecule Adaptation) æ˜¯ä¸€ä¸ªåŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„åˆ†å­-æ–‡æœ¬åŒå‘ç¿»è¯‘æ¡†æ¶ï¼Œé€šè¿‡ä¸Šä¸‹æ–‡åˆ†å­è°ƒä¼˜æ¥æå‡å¤§è¯­è¨€æ¨¡å‹åœ¨åˆ†å­å‘ç°ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚

**è®ºæ–‡é“¾æ¥**: [Large Language Models are In-Context Molecule Learners](https://ieeexplore.ieee.org/abstract/document/10948482/)

## ğŸ“¢ æ–°é—»

* ğŸ‰ **å·²è¢« IEEE TKDE æ¥å—**
* æˆ‘ä»¬å‘å¸ƒäº†åŸºäº Galactica-125M çš„ä¸¤ä¸ªç‰ˆæœ¬çš„ ICMA æ¨¡å‹ï¼Œå¯é€šè¿‡ HuggingFace è®¿é—®ï¼š
  * [phenixace/ICMA-Galactica-125M-M2C](https://huggingface.co/phenixace/ICMA-Galactica-125M-M2C) (åˆ†å­åˆ°æè¿°)
  * [phenixace/ICMA-Galactica-125M-C2M](https://huggingface.co/phenixace/ICMA-Galactica-125M-C2M) (æè¿°åˆ°åˆ†å­)
* å®Œæ•´çš„ä»£ç åº“ç°å·²å‘å¸ƒï¼

## ğŸ¯ é¡¹ç›®ç®€ä»‹

ICMA ä¸“æ³¨äºåˆ†å­å‘ç°ä¸­çš„ä¸¤ä¸ªæ ¸å¿ƒä»»åŠ¡ï¼š

1. **åˆ†å­ç†è§£ (Molecule Understanding)**: ç»™å®šä¸€ä¸ªåˆ†å­ï¼Œç”Ÿæˆæè¿°å…¶ç»“æ„ã€æ€§è´¨å’ŒåŠŸèƒ½çš„æ–‡æœ¬
2. **æ–‡æœ¬æ¡ä»¶åˆ†å­ç”Ÿæˆ (Text-conditioned Molecule Generation)**: æ ¹æ®æ–‡æœ¬æè¿°ç”Ÿæˆå¯¹åº”çš„åˆ†å­ç»“æ„

è¿™ä¸¤ä¸ªä»»åŠ¡åˆ†åˆ«å¯¹åº”ï¼š
- **Mol2Cap (Molecule2Caption)**: ä¸ºç»™å®šåˆ†å­ç”Ÿæˆæè¿°æ–‡æœ¬
- **Cap2Mol (Caption2Molecule)**: æ ¹æ®æè¿°æ–‡æœ¬ç”Ÿæˆå¯¹åº”çš„åˆ†å­

## âœ¨ ä¸»è¦ç‰¹æ€§

### ğŸ§¬ å¤šä»»åŠ¡æ”¯æŒ
- **Mol2Cap**: åˆ†å­åˆ°æ–‡æœ¬æè¿°
- **Cap2Mol**: æ–‡æœ¬æè¿°åˆ°åˆ†å­
- **åˆ†å­æ€§è´¨é¢„æµ‹**: æ”¯æŒå¤šä¸ª MoleculeNet ä»»åŠ¡ï¼ˆBACE, BBBP, ClinTox, HIV, SIDER, Tox21, ToxCastï¼‰

### ğŸ“Š å¤šæ•°æ®é›†æ”¯æŒ
- **ChEBI-20**: åŒ–å­¦å®ä½“ç”Ÿç‰©å­¦å…´è¶£æ•°æ®åº“å­é›†ï¼ˆ26,407 è®­ç»ƒæ ·æœ¬ï¼‰
- **PubChem324k**: PubChem å¤§è§„æ¨¡æ•°æ®é›†ï¼ˆIUPAC å‘½åå’Œåˆ†å­æè¿°ï¼‰
- **MoleculeNet**: å¤šä¸ªåˆ†å­æ€§è´¨é¢„æµ‹ä»»åŠ¡

### ğŸ” æ£€ç´¢å¢å¼ºå­¦ä¹ 
- **å¤šç§æ£€ç´¢æ–¹æ³•**:
  - **Mol2Cap**: GNN ç›¸ä¼¼åº¦ã€Morgan æŒ‡çº¹ã€éšæœºæ£€ç´¢
  - **Cap2Mol**: BM25ã€SentenceBERTã€éšæœºæ£€ç´¢
- **Few-shot å­¦ä¹ **: é€šè¿‡æ£€ç´¢ç›¸ä¼¼æ ·æœ¬è¿›è¡Œä¸Šä¸‹æ–‡å­¦ä¹ 
- **è‡ªåŠ¨ç¤ºä¾‹é€‰æ‹©**: æ™ºèƒ½é€‰æ‹©æœ€ç›¸å…³çš„è®­ç»ƒæ ·æœ¬ä½œä¸ºç¤ºä¾‹

### ğŸš€ é«˜æ•ˆè®­ç»ƒ
- **LoRA é€‚é…å™¨**: å‚æ•°é«˜æ•ˆçš„æ¨¡å‹å¾®è°ƒ
- **æ”¯æŒå¤šç§æ¨¡å‹**: æ”¯æŒ decoder-only å’Œ encoder-decoder æ¶æ„
- **çµæ´»é…ç½®**: æ”¯æŒ 8-bit é‡åŒ–ã€FP16 æ··åˆç²¾åº¦è®­ç»ƒ

## ğŸ“ é¡¹ç›®ç»“æ„

```
ICMA/
â”œâ”€â”€ æ ¸å¿ƒä»£ç 
â”‚   â”œâ”€â”€ icma_train.py      # è®­ç»ƒä¸»ç¨‹åº
â”‚   â”œâ”€â”€ inference.py        # æ¨ç†ä¸»ç¨‹åº
â”‚   â”œâ”€â”€ naive_test.py       # ç®€å•æµ‹è¯•è„šæœ¬
â”‚   â”œâ”€â”€ dataset.py          # æ•°æ®é›†åŠ è½½æ¨¡å—
â”‚   â””â”€â”€ utils.py            # å·¥å…·å‡½æ•°æ¨¡å—
â”‚
â”œâ”€â”€ æ•°æ®å¤„ç† (data/)
â”‚   â”œâ”€â”€ ChEBI-20/          # ChEBI-20æ•°æ®é›†
â”‚   â”œâ”€â”€ PubChem324k/       # PubChemæ•°æ®é›†
â”‚   â””â”€â”€ MoleculeNet/       # MoleculeNetæ•°æ®é›†
â”‚
â”œâ”€â”€ è¯„ä¼°æŒ‡æ ‡ (evaluations/)
â”‚   â”œâ”€â”€ text_translation_metrics.py    # æ–‡æœ¬ç¿»è¯‘æŒ‡æ ‡
â”‚   â”œâ”€â”€ mol_translation_metrics.py     # åˆ†å­ç¿»è¯‘æŒ‡æ ‡
â”‚   â”œâ”€â”€ fingerprint_metrics.py         # æŒ‡çº¹ç›¸ä¼¼åº¦æŒ‡æ ‡
â”‚   â””â”€â”€ ...
â”‚
â””â”€â”€ è„šæœ¬ (run_train.bash)   # è®­ç»ƒè„šæœ¬
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒè¦æ±‚

- Python 3.7+
- PyTorch 2.0+
- Transformers
- PEFT (Parameter-Efficient Fine-Tuning)
- RDKit
- å…¶ä»–ä¾èµ–è§ `requirements.txt`

### å®‰è£…

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/phenixace/ICMA.git
cd ICMA

# å®‰è£…ä¾èµ–
pip install torch transformers peft datasets rdkit sentence-transformers
```

### è®­ç»ƒ

```bash
# ä½¿ç”¨æ£€ç´¢å¢å¼ºçš„ Few-shot å­¦ä¹ è®­ç»ƒ Mol2Cap ä»»åŠ¡
python icma_train.py \
    --base_model "facebook/galactica-125m" \
    --data_folder "./data/ChEBI-20/raw/" \
    --output_dir "./ckp/galactica-125M/mol2cap/" \
    --task "mol2cap" \
    --retrieval \
    --n_shot 1 \
    --m2c_method "gnn" \
    --micro_batch_size 4 \
    --batch_size 32 \
    --num_epochs 10 \
    --learning_rate 2e-5

# è®­ç»ƒ Cap2Mol ä»»åŠ¡
python icma_train.py \
    --base_model "facebook/galactica-125m" \
    --data_folder "./data/ChEBI-20/raw/" \
    --output_dir "./ckp/galactica-125M/cap2mol/" \
    --task "cap2mol" \
    --retrieval \
    --n_shot 1 \
    --c2m_method "bm25" \
    --micro_batch_size 4 \
    --batch_size 32 \
    --num_epochs 10 \
    --learning_rate 2e-5
```

### æ¨ç†

```bash
# ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œæ¨ç†
python inference.py \
    --base_model "facebook/galactica-125m" \
    --adapter_path "./ckp/galactica-125M/mol2cap/checkpoint-8000/" \
    --data_folder "./data/ChEBI-20/raw/" \
    --task "mol2cap" \
    --output_dir "./predictions/galactica-125M/mol2cap/" \
    --retrieval \
    --n_shot 1 \
    --m2c_method "gnn" \
    --batch_infer
```

### è¯„ä¼°

```bash
# ä½¿ç”¨ naive_test.py è¯„ä¼°ç»“æœ
python naive_test.py \
    --raw_folder "./data/ChEBI-20/raw/" \
    --target_folder "./predictions/" \
    --model "galactica-125M" \
    --ckp 8000 \
    --task "mol2cap"
```

## ğŸ”¬ æŠ€æœ¯ç»†èŠ‚

### æ£€ç´¢å¢å¼º Few-shot å­¦ä¹ 

ICMA é€šè¿‡ä»¥ä¸‹æ–¹å¼æå‡æ€§èƒ½ï¼š

1. **ç›¸ä¼¼åº¦æ£€ç´¢**: ä»è®­ç»ƒé›†ä¸­æ£€ç´¢ä¸æŸ¥è¯¢æœ€ç›¸ä¼¼çš„æ ·æœ¬
2. **ä¸Šä¸‹æ–‡æ„å»º**: å°†æ£€ç´¢åˆ°çš„æ ·æœ¬ä½œä¸º few-shot ç¤ºä¾‹æ„å»ºæç¤º
3. **å‚æ•°å¾®è°ƒ**: ä½¿ç”¨ LoRA é€‚é…å™¨å¯¹æ¨¡å‹è¿›è¡Œå‚æ•°é«˜æ•ˆçš„å¾®è°ƒ

### æ”¯æŒçš„æ£€ç´¢æ–¹æ³•

- **Mol2Cap**:
  - `gnn`: åŸºäº GNN çš„åˆ†å­ç›¸ä¼¼åº¦ï¼ˆé»˜è®¤ï¼‰
  - `morgan`: Morgan æŒ‡çº¹ç›¸ä¼¼åº¦
  - `random`: éšæœºæ£€ç´¢

- **Cap2Mol**:
  - `bm25`: BM25 æ–‡æœ¬æ£€ç´¢ï¼ˆé»˜è®¤ï¼‰
  - `sentencebert`: Sentence-BERT è¯­ä¹‰ç›¸ä¼¼åº¦
  - `random`: éšæœºæ£€ç´¢

## ğŸ“ˆ å®éªŒç»“æœ

ICMA åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼š

- åœ¨ ChEBI-20 æ•°æ®é›†ä¸Šï¼ŒICMA åœ¨ Mol2Cap å’Œ Cap2Mol ä»»åŠ¡ä¸Šéƒ½è¶…è¶Šäº†åŸºçº¿æ–¹æ³•
- åœ¨ MoleculeNet æ€§è´¨é¢„æµ‹ä»»åŠ¡ä¸Šï¼ŒICMA å±•ç°äº†è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ Pull Requestï¼

## ğŸ“š å¼•ç”¨

å¦‚æœæ‚¨åœ¨ç ”ç©¶ä¸­ä½¿ç”¨ ICMAï¼Œè¯·å¼•ç”¨æˆ‘ä»¬çš„è®ºæ–‡ï¼š

```bibtex
@article{li2025large,
  title={Large language models are in-context molecule learners},
  author={Li, Jiatong and Liu, Wei and Ding, Zhihao and Fan, Wenqi and Li, Yuqiang and Li, Qing},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  year={2025},
  publisher={IEEE}
}
```

```bibtex
@article{li2024empowering,
  title={Empowering molecule discovery for molecule-caption translation with large language models: A chatgpt perspective},
  author={Li, Jiatong and Liu, Yunqing and Fan, Wenqi and Wei, Xiao-Yong and Liu, Hui and Tang, Jiliang and Li, Qing},
  journal={IEEE transactions on knowledge and data engineering},
  volume={36},
  number={11},
  pages={6071--6083},
  year={2024},
  publisher={IEEE}
}
```

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®éµå¾ªç›¸åº”çš„å¼€æºè®¸å¯è¯ã€‚

## ğŸ“§ è”ç³»æ–¹å¼

å¦‚æœ‰é—®é¢˜ï¼Œè¯·é€šè¿‡ä»¥ä¸‹æ–¹å¼è”ç³»ï¼š
- æäº¤ GitHub Issue
- å‘é€é‚®ä»¶è‡³é¡¹ç›®ç»´æŠ¤è€…

## ğŸ™ è‡´è°¢

æ„Ÿè°¢æ‰€æœ‰ä¸ºæœ¬é¡¹ç›®åšå‡ºè´¡çŒ®çš„ç ”ç©¶è€…å’Œå¼€å‘è€…ã€‚
