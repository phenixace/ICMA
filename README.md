# ICMA: Large Language Models are In-Context Molecule Learners

The official repo of `ICMA` **"Large Language Models are In-Context Molecule Learners"**. ([Available through Arxiv Link](https://arxiv.org/abs/2403.04197)) 

## News
* We release two versions of ICMA with the backbone of Galactica-125M, You could acess the model card via huggingface [phenixace/ICMA-Galactica-125M-M2C](https://huggingface.co/phenixace/ICMA-Galactica-125M-M2C) and [phenixace/ICMA-Galactica-125M-C2M](https://huggingface.co/phenixace/ICMA-Galactica-125M-C2M).
* The codes will be available soon~ We will follow our roadmap to release the results. Please stay tuned!
